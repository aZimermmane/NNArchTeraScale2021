{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML (run this only once, this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's print some information about the dataset\n",
    "Print the feature names and the dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zlogz', 'c1_b0_mmdt', 'c1_b1_mmdt', 'c1_b2_mmdt', 'c2_b1_mmdt', 'c2_b2_mmdt', 'd2_b1_mmdt', 'd2_b2_mmdt', 'd2_a1_b1_mmdt', 'd2_a1_b2_mmdt', 'm2_b1_mmdt', 'm2_b2_mmdt', 'n2_b1_mmdt', 'n2_b2_mmdt', 'mass_mmdt', 'multiplicity']\n",
      "(830000, 16) (830000,)\n",
      "[[-2.93512535e+00  3.83155316e-01  5.12587558e-03  8.42466834e-05\n",
      "   9.06995591e-03  1.78931368e-04  1.76944518e+00  2.12389827e+00\n",
      "   1.76944518e+00  3.08185428e-01  1.35686919e-01  8.32780078e-02\n",
      "   4.12136108e-01  2.99057871e-01  8.92688179e+00  7.50000000e+01]\n",
      " [-1.92733514e+00  2.70698756e-01  1.58540264e-03  1.13709866e-05\n",
      "   3.23237223e-03  2.91449633e-05  2.03883362e+00  2.56309891e+00\n",
      "   2.03883362e+00  2.11886495e-01  6.37285784e-02  3.63104008e-02\n",
      "   3.10216516e-01  2.26661310e-01  3.88651156e+00  3.10000000e+01]\n",
      " [-3.11214662e+00  4.58171129e-01  9.79138538e-02  2.85884105e-02\n",
      "   1.24277540e-01  3.84868123e-02  1.26925385e+00  1.34623826e+00\n",
      "   1.26925385e+00  2.46488109e-01  1.15635969e-01  7.90941268e-02\n",
      "   3.57558519e-01  2.89219588e-01  1.62144669e+02  6.10000000e+01]\n",
      " [-2.66651511e+00  4.37067598e-01  4.91220504e-02  7.97829404e-03\n",
      "   4.74766865e-02  4.80184611e-03  9.66504514e-01  6.01863742e-01\n",
      "   9.66504514e-01  1.60756409e-01  8.21964443e-02  3.33105028e-02\n",
      "   2.38870576e-01  9.45160612e-02  9.12589340e+01  3.90000000e+01]\n",
      " [-2.48484278e+00  4.28981334e-01  4.17859703e-02  6.10950543e-03\n",
      "   2.30659451e-02  1.12305430e-03  5.52002132e-01  1.83820814e-01\n",
      "   5.52002132e-01  8.43379796e-02  4.80063371e-02  1.44504104e-02\n",
      "   1.41905606e-01  3.66654508e-02  7.97257767e+01  3.50000000e+01]]\n",
      "['g' 'w' 't' 'z' 'w' 'w' 't' 'g' 'z' 'g']\n"
     ]
    }
   ],
   "source": [
    "print(data['feature_names'])\n",
    "print(X.shape, y.shape)\n",
    "print(X[:5])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw above, the `y` target is an array of strings, e.g. \\['g', 'w',...\\] etc.\n",
    "We need to make this a \"One Hot\" encoding for the training.\n",
    "Then, split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g' 'q' 't' 'w' 'z']\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2 4 3 3 2 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "yl = le.transform(y)\n",
    "print(yl[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "yc = to_categorical(yl, len(le.classes_))\n",
    "print(yc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, yc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def History(label, h):\n",
    "    plt.plot( h.history['loss'], label ='Training Loss: {}'.format(label))\n",
    "    plt.plot( h.history['val_loss'], label = 'Validation Loss: {}'.format(label))\n",
    "    plt.plot( h.history['accuracy'], label = 'Training Accuracy: {}'.format(label))\n",
    "    plt.plot( h.history['val_accuracy'], label = 'Validation Accuracy: {}'.format(label))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(m):\n",
    "    pred = m.predict( X_test)\n",
    "    a = accuracy_score(np.argmax(y_test, axis=1), np.argmax(pred, axis=1))\n",
    "    print(\"Accuracy: {}: {}\".format(m.name, a))\n",
    "    return pred, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a simple dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 3 hidden layers with 32, then 64, then 64 neurons. Each layer will use `relu` activation.\n",
    "Add an output layer with 5 neurons (one for each class), then finish with Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense, Activation, BatchNormalization, Conv1D, Dropout, Reshape, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(16,))\n",
    "d = Dense(32, name='fc1', activation='relu')(i)\n",
    "d = Dense(64, name='fc2', activation='relu')(d)\n",
    "d = Dense(64, name='fc3', activation='relu')(d)\n",
    "o = Dense(len(le.classes_), activation='softmax', name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(d)\n",
    "model_1 = Model(inputs=i, outputs=o, name='model_DNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_DNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 7,141\n",
      "Trainable params: 7,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use Adam optimizer with categorical crossentropy loss.\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model_1.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "487/487 [==============================] - 1s 3ms/step - loss: 1.8687 - accuracy: 0.3806 - val_loss: 1.3102 - val_accuracy: 0.4289\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 1.2659 - accuracy: 0.4822 - val_loss: 1.2186 - val_accuracy: 0.5288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f17301aeef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_train_val, y_train_val, batch_size=1024,\n",
    "              epochs=2, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: model_DNN: 0.5287048192771084\n"
     ]
    }
   ],
   "source": [
    "y_predict_1, a_1 = Accuracy(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(16,))\n",
    "r = Reshape((16,1))(i)\n",
    "c = Conv1D(16, 3, name='cv1', activation='relu')(r)\n",
    "c = Conv1D(16, 3, name='cv2', activation='relu')(c)\n",
    "c = Conv1D(16, 3, name='cv3', activation='relu')(c)\n",
    "f = Flatten()(c)\n",
    "o = Dense(len(le.classes_), activation='softmax', name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(f)\n",
    "model_2 = Model(inputs=i, outputs=o, name='model_CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 1)             0         \n",
      "_________________________________________________________________\n",
      "cv1 (Conv1D)                 (None, 14, 16)            64        \n",
      "_________________________________________________________________\n",
      "cv2 (Conv1D)                 (None, 12, 16)            784       \n",
      "_________________________________________________________________\n",
      "cv3 (Conv1D)                 (None, 10, 16)            784       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 805       \n",
      "=================================================================\n",
      "Total params: 2,437\n",
      "Trainable params: 2,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use Adam optimizer with categorical crossentropy loss.\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model_2.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "487/487 [==============================] - 2s 4ms/step - loss: 1.4583 - accuracy: 0.3559 - val_loss: 1.2897 - val_accuracy: 0.4795\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 2s 3ms/step - loss: 1.2041 - accuracy: 0.5502 - val_loss: 1.1561 - val_accuracy: 0.5681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f17204af588>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(X_train_val, y_train_val, batch_size=1024,\n",
    "              epochs=2, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: model_CNN: 0.5687951807228916\n"
     ]
    }
   ],
   "source": [
    "y_predict_2, a_2 = Accuracy(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the best model then ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best model is: model_CNN\n"
     ]
    }
   ],
   "source": [
    "print(\"the best model is: {}\".format(model_1.name if a_1 > a_2 else model_2.name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
