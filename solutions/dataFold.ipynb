{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML (run this only once, this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "yl = le.fit_transform(y)\n",
    "yc = to_categorical(yl, len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, yc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def History(label, h):\n",
    "    plt.plot( h.history['loss'], label ='Training Loss: {}'.format(label))\n",
    "    plt.plot( h.history['val_loss'], label = 'Validation Loss: {}'.format(label))\n",
    "    plt.plot( h.history['accuracy'], label = 'Training Accuracy: {}'.format(label))\n",
    "    plt.plot( h.history['val_accuracy'], label = 'Validation Accuracy: {}'.format(label))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(m, X, Y):\n",
    "    pred = m.predict( X)\n",
    "    a = accuracy_score(np.argmax(Y, axis=1), np.argmax(pred, axis=1))\n",
    "    print(\"Accuracy: {}: {}\".format(m.name, a))\n",
    "    return pred, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a simple dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 3 hidden layers with 32, then 64, then 64 neurons. Each layer will use `relu` activation.\n",
    "Add an output layer with 5 neurons (one for each class), then finish with Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense, Activation, BatchNormalization, Conv1D, Dropout, Reshape, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(16,))\n",
    "d = Dense(32, name='fc1', activation='relu')(i)\n",
    "d = Dense(64, name='fc2', activation='relu')(d)\n",
    "d = Dense(64, name='fc3', activation='relu')(d)\n",
    "o = Dense(len(le.classes_), activation='softmax', name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(d)\n",
    "model = Model(inputs=i, outputs=o, name='model_DNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_DNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 7,141\n",
      "Trainable params: 7,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use Adam optimizer with categorical crossentropy loss.\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "folding = StratifiedKFold(n_splits=n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "548/548 [==============================] - 9s 16ms/step - loss: 1.5251 - accuracy: 0.3539 - val_loss: 1.3430 - val_accuracy: 0.4088\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 7s 12ms/step - loss: 1.3075 - accuracy: 0.4407 - val_loss: 1.2641 - val_accuracy: 0.4789\n",
      "Accuracy: model_DNN_0: 0.4762409638554217\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 7s 13ms/step - loss: 1.2115 - accuracy: 0.5403 - val_loss: 1.1558 - val_accuracy: 0.5839\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 7s 13ms/step - loss: 1.1147 - accuracy: 0.5941 - val_loss: 1.0791 - val_accuracy: 0.6055\n",
      "Accuracy: model_DNN_1: 0.6050481927710843\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 7s 12ms/step - loss: 1.0549 - accuracy: 0.6134 - val_loss: 1.0352 - val_accuracy: 0.6242\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 6s 10ms/step - loss: 1.0213 - accuracy: 0.6310 - val_loss: 1.0091 - val_accuracy: 0.6397\n",
      "Accuracy: model_DNN_2: 0.6377710843373494\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 7s 12ms/step - loss: 0.9985 - accuracy: 0.6425 - val_loss: 0.9883 - val_accuracy: 0.6498\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 7s 12ms/step - loss: 0.9813 - accuracy: 0.6502 - val_loss: 0.9732 - val_accuracy: 0.6523\n",
      "Accuracy: model_DNN_3: 0.652\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 4s 8ms/step - loss: 0.9678 - accuracy: 0.6563 - val_loss: 0.9621 - val_accuracy: 0.6622\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 4s 8ms/step - loss: 0.9560 - accuracy: 0.6619 - val_loss: 0.9503 - val_accuracy: 0.6646\n",
      "Accuracy: model_DNN_4: 0.6645903614457831\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 6s 11ms/step - loss: 0.9463 - accuracy: 0.6657 - val_loss: 0.9438 - val_accuracy: 0.6662\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 5s 9ms/step - loss: 0.9373 - accuracy: 0.6699 - val_loss: 0.9323 - val_accuracy: 0.6738\n",
      "Accuracy: model_DNN_5: 0.6712771084337349\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 1s 3ms/step - loss: 0.9279 - accuracy: 0.6737 - val_loss: 0.9241 - val_accuracy: 0.6785\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 1s 2ms/step - loss: 0.9196 - accuracy: 0.6773 - val_loss: 0.9156 - val_accuracy: 0.6811\n",
      "Accuracy: model_DNN_6: 0.6786626506024096\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 1s 3ms/step - loss: 0.9126 - accuracy: 0.6805 - val_loss: 0.9113 - val_accuracy: 0.6844\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 1s 2ms/step - loss: 0.9056 - accuracy: 0.6840 - val_loss: 0.9043 - val_accuracy: 0.6872\n",
      "Accuracy: model_DNN_7: 0.6859156626506024\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 1s 3ms/step - loss: 0.8983 - accuracy: 0.6870 - val_loss: 0.8940 - val_accuracy: 0.6895\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 1s 2ms/step - loss: 0.8916 - accuracy: 0.6902 - val_loss: 0.8873 - val_accuracy: 0.6925\n",
      "Accuracy: model_DNN_8: 0.6920963855421687\n",
      "Epoch 1/2\n",
      "548/548 [==============================] - 1s 3ms/step - loss: 0.8852 - accuracy: 0.6928 - val_loss: 0.8824 - val_accuracy: 0.6927\n",
      "Epoch 2/2\n",
      "548/548 [==============================] - 1s 2ms/step - loss: 0.8791 - accuracy: 0.6956 - val_loss: 0.8775 - val_accuracy: 0.6964\n",
      "Accuracy: model_DNN_9: 0.6978313253012048\n"
     ]
    }
   ],
   "source": [
    "histories=[]\n",
    "accuracies=[]\n",
    "models=[]\n",
    "for i_fold, (train_index, test_index) in enumerate(folding.split(X, y)):\n",
    "    X_fold_train, X_fold_test = X[train_index], X[test_index]\n",
    "    y_fold_train, y_fold_test = yc[train_index], yc[test_index]\n",
    "    models.append( Model(inputs=i, outputs=o, name='model_DNN_{}'.format(i_fold)) )\n",
    "    models[-1].compile(optimizer=Adam(lr=0.0001), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    histories.append(models[-1].fit( X_fold_train, y_fold_train, batch_size=1024,\n",
    "                    epochs=2, validation_split=0.25, shuffle=True) )\n",
    "    _, a = Accuracy(models[-1], X_fold_test, y_fold_test)\n",
    "    accuracies.append( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy is 0.646 +/- 0.062 over 10 folds\n"
     ]
    }
   ],
   "source": [
    "print(\"The model accuracy is {:0.3f} +/- {:0.3f} over {} folds\".format(np.mean(accuracies),np.std(accuracies), n_folds ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Implement this mechanism to the starter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "Can the number of folds be reduced as the RMS on performance is reducing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
