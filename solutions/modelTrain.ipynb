{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML (run this only once, this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "yl = le.fit_transform(y)\n",
    "yc = to_categorical(yl, len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, yc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def History(label, h):\n",
    "    plt.plot( h.history['loss'], label ='Training Loss: {}'.format(label))\n",
    "    plt.plot( h.history['val_loss'], label = 'Validation Loss: {}'.format(label))\n",
    "    plt.plot( h.history['accuracy'], label = 'Training Accuracy: {}'.format(label))\n",
    "    plt.plot( h.history['val_accuracy'], label = 'Validation Accuracy: {}'.format(label))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(m):\n",
    "    pred = m.predict( X_test)\n",
    "    a = accuracy_score(np.argmax(y_test, axis=1), np.argmax(pred, axis=1))\n",
    "    print(\"Accuracy: {}: {}\".format(m.name, a))\n",
    "    return pred, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a simple dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 3 hidden layers with 32, then 64, then 64 neurons. Each layer will use `relu` activation.\n",
    "Add an output layer with 5 neurons (one for each class), then finish with Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense, Activation, BatchNormalization, Conv1D, Dropout, Reshape, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(16,))\n",
    "d = Dense(32, name='fc1', activation='relu')(i)\n",
    "d = Dense(64, name='fc2', activation='relu')(d)\n",
    "d = Dense(64, name='fc3', activation='relu')(d)\n",
    "o = Dense(len(le.classes_), activation='softmax', name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(d)\n",
    "model = Model(inputs=i, outputs=o, name='model_DNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_DNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 7,141\n",
      "Trainable params: 7,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use Adam optimizer with categorical crossentropy loss.\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint,TensorBoard,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping = EarlyStopping(monitor='val_loss', \n",
    "                         patience=10, \n",
    "                         verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, \n",
    "                              mode='min', verbose=1, min_delta=0.001,\n",
    "                              cooldown=4, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "487/487 [==============================] - 4s 8ms/step - loss: 1.2021 - accuracy: 0.5330 - val_loss: 1.0206 - val_accuracy: 0.6494\n",
      "Epoch 2/1000\n",
      "487/487 [==============================] - 2s 5ms/step - loss: 0.9672 - accuracy: 0.6588 - val_loss: 0.9450 - val_accuracy: 0.6667\n",
      "Epoch 3/1000\n",
      "487/487 [==============================] - 4s 8ms/step - loss: 0.9169 - accuracy: 0.6782 - val_loss: 0.9061 - val_accuracy: 0.6858\n",
      "Epoch 4/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.8842 - accuracy: 0.6918 - val_loss: 0.8920 - val_accuracy: 0.6885\n",
      "Epoch 5/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.8548 - accuracy: 0.7025 - val_loss: 0.8531 - val_accuracy: 0.7073\n",
      "Epoch 6/1000\n",
      "487/487 [==============================] - 6s 12ms/step - loss: 0.8313 - accuracy: 0.7111 - val_loss: 0.8214 - val_accuracy: 0.7159\n",
      "Epoch 7/1000\n",
      "487/487 [==============================] - 5s 9ms/step - loss: 0.8145 - accuracy: 0.7164 - val_loss: 0.8131 - val_accuracy: 0.7184\n",
      "Epoch 8/1000\n",
      "487/487 [==============================] - 7s 14ms/step - loss: 0.8041 - accuracy: 0.7189 - val_loss: 0.8117 - val_accuracy: 0.7178\n",
      "Epoch 9/1000\n",
      "487/487 [==============================] - 7s 14ms/step - loss: 0.7957 - accuracy: 0.7209 - val_loss: 0.8448 - val_accuracy: 0.7028\n",
      "Epoch 10/1000\n",
      "487/487 [==============================] - 5s 11ms/step - loss: 0.7902 - accuracy: 0.7223 - val_loss: 0.7903 - val_accuracy: 0.7242\n",
      "Epoch 11/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7849 - accuracy: 0.7231 - val_loss: 0.7847 - val_accuracy: 0.7256\n",
      "Epoch 12/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7807 - accuracy: 0.7244 - val_loss: 0.7870 - val_accuracy: 0.7249\n",
      "Epoch 13/1000\n",
      "487/487 [==============================] - 6s 12ms/step - loss: 0.7774 - accuracy: 0.7253 - val_loss: 0.7805 - val_accuracy: 0.7258\n",
      "Epoch 14/1000\n",
      "487/487 [==============================] - 6s 12ms/step - loss: 0.7772 - accuracy: 0.7254 - val_loss: 0.7726 - val_accuracy: 0.7275\n",
      "Epoch 15/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7735 - accuracy: 0.7264 - val_loss: 0.7771 - val_accuracy: 0.7258\n",
      "Epoch 16/1000\n",
      "487/487 [==============================] - 6s 12ms/step - loss: 0.7722 - accuracy: 0.7261 - val_loss: 0.7701 - val_accuracy: 0.7290\n",
      "Epoch 17/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7693 - accuracy: 0.7272 - val_loss: 0.7788 - val_accuracy: 0.7254\n",
      "Epoch 18/1000\n",
      "487/487 [==============================] - 5s 11ms/step - loss: 0.7669 - accuracy: 0.7277 - val_loss: 0.7667 - val_accuracy: 0.7284\n",
      "Epoch 19/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7649 - accuracy: 0.7282 - val_loss: 0.7751 - val_accuracy: 0.7258\n",
      "Epoch 20/1000\n",
      "487/487 [==============================] - 4s 8ms/step - loss: 0.7636 - accuracy: 0.7290 - val_loss: 0.7794 - val_accuracy: 0.7262\n",
      "Epoch 21/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7630 - accuracy: 0.7288 - val_loss: 0.7609 - val_accuracy: 0.7312\n",
      "Epoch 22/1000\n",
      "487/487 [==============================] - 6s 11ms/step - loss: 0.7600 - accuracy: 0.7298 - val_loss: 0.7626 - val_accuracy: 0.7312\n",
      "Epoch 23/1000\n",
      "487/487 [==============================] - 7s 14ms/step - loss: 0.7586 - accuracy: 0.7304 - val_loss: 0.7583 - val_accuracy: 0.7318\n",
      "Epoch 24/1000\n",
      "487/487 [==============================] - 7s 15ms/step - loss: 0.7562 - accuracy: 0.7313 - val_loss: 0.7590 - val_accuracy: 0.7321\n",
      "Epoch 25/1000\n",
      "487/487 [==============================] - 4s 8ms/step - loss: 0.7545 - accuracy: 0.7318 - val_loss: 0.7529 - val_accuracy: 0.7340\n",
      "Epoch 26/1000\n",
      "487/487 [==============================] - 3s 6ms/step - loss: 0.7543 - accuracy: 0.7317 - val_loss: 0.8047 - val_accuracy: 0.7165\n",
      "Epoch 27/1000\n",
      "487/487 [==============================] - 4s 7ms/step - loss: 0.7520 - accuracy: 0.7320 - val_loss: 0.7635 - val_accuracy: 0.7317\n",
      "Epoch 28/1000\n",
      "487/487 [==============================] - 4s 8ms/step - loss: 0.7488 - accuracy: 0.7337 - val_loss: 0.7578 - val_accuracy: 0.7316\n",
      "Epoch 29/1000\n",
      "487/487 [==============================] - 4s 8ms/step - loss: 0.7474 - accuracy: 0.7341 - val_loss: 0.7515 - val_accuracy: 0.7318\n",
      "Epoch 30/1000\n",
      "487/487 [==============================] - 3s 7ms/step - loss: 0.7443 - accuracy: 0.7353 - val_loss: 0.7448 - val_accuracy: 0.7372\n",
      "Epoch 31/1000\n",
      "487/487 [==============================] - 4s 9ms/step - loss: 0.7422 - accuracy: 0.7363 - val_loss: 0.7431 - val_accuracy: 0.7368\n",
      "Epoch 32/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7390 - accuracy: 0.7377 - val_loss: 0.7390 - val_accuracy: 0.7366\n",
      "Epoch 33/1000\n",
      "487/487 [==============================] - 6s 12ms/step - loss: 0.7351 - accuracy: 0.7389 - val_loss: 0.7375 - val_accuracy: 0.7397\n",
      "Epoch 34/1000\n",
      "487/487 [==============================] - 6s 13ms/step - loss: 0.7346 - accuracy: 0.7392 - val_loss: 0.7453 - val_accuracy: 0.7350\n",
      "Epoch 35/1000\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7305 - accuracy: 0.7406 - val_loss: 0.7340 - val_accuracy: 0.7403\n",
      "Epoch 36/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7289 - accuracy: 0.7412 - val_loss: 0.7321 - val_accuracy: 0.7406\n",
      "Epoch 37/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7268 - accuracy: 0.7425 - val_loss: 0.7319 - val_accuracy: 0.7407\n",
      "Epoch 38/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7251 - accuracy: 0.7425 - val_loss: 0.7256 - val_accuracy: 0.7444\n",
      "Epoch 39/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7251 - accuracy: 0.7425 - val_loss: 0.7273 - val_accuracy: 0.7418\n",
      "Epoch 40/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7222 - accuracy: 0.7437 - val_loss: 0.7250 - val_accuracy: 0.7439\n",
      "Epoch 41/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7214 - accuracy: 0.7440 - val_loss: 0.7257 - val_accuracy: 0.7442\n",
      "Epoch 42/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7181 - accuracy: 0.7453 - val_loss: 0.7123 - val_accuracy: 0.7475\n",
      "Epoch 43/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7157 - accuracy: 0.7464 - val_loss: 0.7112 - val_accuracy: 0.7491\n",
      "Epoch 44/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7145 - accuracy: 0.7465 - val_loss: 0.7252 - val_accuracy: 0.7447\n",
      "Epoch 45/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7138 - accuracy: 0.7468 - val_loss: 0.7194 - val_accuracy: 0.7452\n",
      "Epoch 46/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7102 - accuracy: 0.7481 - val_loss: 0.7064 - val_accuracy: 0.7510\n",
      "Epoch 47/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7084 - accuracy: 0.7487 - val_loss: 0.7240 - val_accuracy: 0.7433\n",
      "Epoch 48/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7080 - accuracy: 0.7491 - val_loss: 0.7176 - val_accuracy: 0.7483\n",
      "Epoch 49/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7058 - accuracy: 0.7497 - val_loss: 0.7221 - val_accuracy: 0.7458\n",
      "Epoch 50/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7080 - accuracy: 0.7488 - val_loss: 0.7083 - val_accuracy: 0.7492\n",
      "Epoch 51/1000\n",
      "474/487 [============================>.] - ETA: 0s - loss: 0.7074 - accuracy: 0.7485\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.7075 - accuracy: 0.7485 - val_loss: 0.7081 - val_accuracy: 0.7491\n",
      "Epoch 52/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6953 - accuracy: 0.7531 - val_loss: 0.7176 - val_accuracy: 0.7447\n",
      "Epoch 53/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.7538 - val_loss: 0.6964 - val_accuracy: 0.7530\n",
      "Epoch 54/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.7545 - val_loss: 0.6954 - val_accuracy: 0.7536\n",
      "Epoch 55/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.7544 - val_loss: 0.6969 - val_accuracy: 0.7538\n",
      "Epoch 56/1000\n",
      "487/487 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.7547 - val_loss: 0.6996 - val_accuracy: 0.7539\n",
      "Epoch 57/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.7540 - val_loss: 0.7000 - val_accuracy: 0.7528\n",
      "Epoch 58/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6921 - accuracy: 0.7543 - val_loss: 0.7020 - val_accuracy: 0.7527\n",
      "Epoch 59/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6899 - accuracy: 0.7552 - val_loss: 0.6937 - val_accuracy: 0.7546\n",
      "Epoch 60/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6906 - accuracy: 0.7548 - val_loss: 0.6904 - val_accuracy: 0.7559\n",
      "Epoch 61/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6886 - accuracy: 0.7556 - val_loss: 0.6942 - val_accuracy: 0.7552\n",
      "Epoch 62/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6895 - accuracy: 0.7554 - val_loss: 0.6961 - val_accuracy: 0.7532\n",
      "Epoch 63/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6895 - accuracy: 0.7555 - val_loss: 0.6964 - val_accuracy: 0.7545\n",
      "Epoch 64/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6888 - accuracy: 0.7554 - val_loss: 0.6920 - val_accuracy: 0.7557\n",
      "Epoch 65/1000\n",
      "464/487 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.7555\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6888 - accuracy: 0.7555 - val_loss: 0.6938 - val_accuracy: 0.7536\n",
      "Epoch 66/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6826 - accuracy: 0.7574 - val_loss: 0.6872 - val_accuracy: 0.7567\n",
      "Epoch 67/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6833 - accuracy: 0.7572 - val_loss: 0.6876 - val_accuracy: 0.7572\n",
      "Epoch 68/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6830 - accuracy: 0.7574 - val_loss: 0.6876 - val_accuracy: 0.7574\n",
      "Epoch 69/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6825 - accuracy: 0.7574 - val_loss: 0.6883 - val_accuracy: 0.7564\n",
      "Epoch 70/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6825 - accuracy: 0.7574 - val_loss: 0.6867 - val_accuracy: 0.7570\n",
      "Epoch 71/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6824 - accuracy: 0.7574 - val_loss: 0.6862 - val_accuracy: 0.7576\n",
      "Epoch 72/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6830 - accuracy: 0.7575 - val_loss: 0.6988 - val_accuracy: 0.7515\n",
      "Epoch 73/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.7577 - val_loss: 0.6862 - val_accuracy: 0.7572\n",
      "Epoch 74/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6820 - accuracy: 0.7576 - val_loss: 0.6852 - val_accuracy: 0.7571\n",
      "Epoch 75/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.7577 - val_loss: 0.6901 - val_accuracy: 0.7566\n",
      "Epoch 76/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6814 - accuracy: 0.7580 - val_loss: 0.6870 - val_accuracy: 0.7570\n",
      "Epoch 77/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.7576 - val_loss: 0.6888 - val_accuracy: 0.7560\n",
      "Epoch 78/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6815 - accuracy: 0.7582 - val_loss: 0.6844 - val_accuracy: 0.7569\n",
      "Epoch 79/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.7581 - val_loss: 0.7052 - val_accuracy: 0.7521\n",
      "Epoch 80/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6805 - accuracy: 0.7581 - val_loss: 0.7010 - val_accuracy: 0.7512\n",
      "Epoch 81/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6816 - accuracy: 0.7581 - val_loss: 0.6958 - val_accuracy: 0.7529\n",
      "Epoch 82/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6797 - accuracy: 0.7587 - val_loss: 0.6848 - val_accuracy: 0.7575\n",
      "Epoch 83/1000\n",
      "464/487 [===========================>..] - ETA: 0s - loss: 0.6807 - accuracy: 0.7581\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6806 - accuracy: 0.7581 - val_loss: 0.6842 - val_accuracy: 0.7587\n",
      "Epoch 84/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6778 - accuracy: 0.7591 - val_loss: 0.6829 - val_accuracy: 0.7578\n",
      "Epoch 85/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6781 - accuracy: 0.7591 - val_loss: 0.6826 - val_accuracy: 0.7584\n",
      "Epoch 86/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6780 - accuracy: 0.7592 - val_loss: 0.6938 - val_accuracy: 0.7550\n",
      "Epoch 87/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6783 - accuracy: 0.7588 - val_loss: 0.6898 - val_accuracy: 0.7570\n",
      "Epoch 88/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6777 - accuracy: 0.7590 - val_loss: 0.6861 - val_accuracy: 0.7567\n",
      "Epoch 89/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6777 - accuracy: 0.7591 - val_loss: 0.6858 - val_accuracy: 0.7580\n",
      "Epoch 90/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6781 - accuracy: 0.7591 - val_loss: 0.6868 - val_accuracy: 0.7579\n",
      "Epoch 91/1000\n",
      "472/487 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.7594\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6776 - accuracy: 0.7593 - val_loss: 0.6834 - val_accuracy: 0.7585\n",
      "Epoch 92/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6760 - accuracy: 0.7597 - val_loss: 0.6859 - val_accuracy: 0.7566\n",
      "Epoch 93/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6765 - accuracy: 0.7595 - val_loss: 0.6822 - val_accuracy: 0.7585\n",
      "Epoch 94/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6758 - accuracy: 0.7597 - val_loss: 0.6838 - val_accuracy: 0.7572\n",
      "Epoch 95/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6760 - accuracy: 0.7597 - val_loss: 0.6818 - val_accuracy: 0.7584\n",
      "Epoch 96/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6760 - accuracy: 0.7597 - val_loss: 0.6816 - val_accuracy: 0.7589\n",
      "Epoch 97/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.7597 - val_loss: 0.6822 - val_accuracy: 0.7586\n",
      "Epoch 98/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.7597 - val_loss: 0.6816 - val_accuracy: 0.7592\n",
      "Epoch 99/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6758 - accuracy: 0.7595 - val_loss: 0.6819 - val_accuracy: 0.7589\n",
      "Epoch 100/1000\n",
      "473/487 [============================>.] - ETA: 0s - loss: 0.6758 - accuracy: 0.7595\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.7596 - val_loss: 0.6815 - val_accuracy: 0.7588\n",
      "Epoch 101/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6751 - accuracy: 0.7599 - val_loss: 0.6815 - val_accuracy: 0.7587\n",
      "Epoch 102/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6750 - accuracy: 0.7600 - val_loss: 0.6814 - val_accuracy: 0.7590\n",
      "Epoch 103/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6751 - accuracy: 0.7600 - val_loss: 0.6815 - val_accuracy: 0.7589\n",
      "Epoch 104/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6751 - accuracy: 0.7601 - val_loss: 0.6811 - val_accuracy: 0.7590\n",
      "Epoch 105/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6749 - accuracy: 0.7600 - val_loss: 0.6812 - val_accuracy: 0.7588\n",
      "Epoch 106/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6749 - accuracy: 0.7600 - val_loss: 0.6818 - val_accuracy: 0.7592\n",
      "Epoch 107/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6749 - accuracy: 0.7601 - val_loss: 0.6810 - val_accuracy: 0.7589\n",
      "Epoch 108/1000\n",
      "469/487 [===========================>..] - ETA: 0s - loss: 0.6747 - accuracy: 0.7602\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6750 - accuracy: 0.7602 - val_loss: 0.6818 - val_accuracy: 0.7582\n",
      "Epoch 109/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6745 - accuracy: 0.7602 - val_loss: 0.6811 - val_accuracy: 0.7586\n",
      "Epoch 110/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6746 - accuracy: 0.7603 - val_loss: 0.6812 - val_accuracy: 0.7587\n",
      "Epoch 111/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6745 - accuracy: 0.7602 - val_loss: 0.6812 - val_accuracy: 0.7593\n",
      "Epoch 112/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7601 - val_loss: 0.6810 - val_accuracy: 0.7587\n",
      "Epoch 113/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7603 - val_loss: 0.6811 - val_accuracy: 0.7590\n",
      "Epoch 114/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7600 - val_loss: 0.6809 - val_accuracy: 0.7591\n",
      "Epoch 115/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7602 - val_loss: 0.6810 - val_accuracy: 0.7591\n",
      "Epoch 116/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7602 - val_loss: 0.6808 - val_accuracy: 0.7589\n",
      "Epoch 117/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7602 - val_loss: 0.6808 - val_accuracy: 0.7590\n",
      "Epoch 118/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7603 - val_loss: 0.6811 - val_accuracy: 0.7585\n",
      "Epoch 119/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7602 - val_loss: 0.6807 - val_accuracy: 0.7590\n",
      "Epoch 120/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6743 - accuracy: 0.7602 - val_loss: 0.6812 - val_accuracy: 0.7593\n",
      "Epoch 121/1000\n",
      "487/487 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.7602\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6744 - accuracy: 0.7602 - val_loss: 0.6825 - val_accuracy: 0.7589\n",
      "Epoch 122/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7602 - val_loss: 0.6812 - val_accuracy: 0.7586\n",
      "Epoch 123/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6742 - accuracy: 0.7602 - val_loss: 0.6807 - val_accuracy: 0.7589\n",
      "Epoch 124/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7603 - val_loss: 0.6807 - val_accuracy: 0.7590\n",
      "Epoch 125/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7602 - val_loss: 0.6807 - val_accuracy: 0.7587\n",
      "Epoch 126/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7602 - val_loss: 0.6808 - val_accuracy: 0.7591\n",
      "Epoch 127/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7602 - val_loss: 0.6807 - val_accuracy: 0.7592\n",
      "Epoch 128/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7604 - val_loss: 0.6811 - val_accuracy: 0.7585\n",
      "Epoch 129/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7603 - val_loss: 0.6810 - val_accuracy: 0.7590\n",
      "Epoch 130/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7602 - val_loss: 0.6807 - val_accuracy: 0.7590\n",
      "Epoch 131/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7603 - val_loss: 0.6806 - val_accuracy: 0.7589\n",
      "Epoch 132/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7604 - val_loss: 0.6806 - val_accuracy: 0.7593\n",
      "Epoch 133/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7604 - val_loss: 0.6808 - val_accuracy: 0.7591\n",
      "Epoch 134/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7603 - val_loss: 0.6808 - val_accuracy: 0.7587\n",
      "Epoch 135/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7603 - val_loss: 0.6809 - val_accuracy: 0.7592\n",
      "Epoch 136/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7605 - val_loss: 0.6810 - val_accuracy: 0.7593\n",
      "Epoch 137/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7602 - val_loss: 0.6807 - val_accuracy: 0.7590\n",
      "Epoch 138/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7603 - val_loss: 0.6809 - val_accuracy: 0.7593\n",
      "Epoch 139/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7604 - val_loss: 0.6807 - val_accuracy: 0.7588\n",
      "Epoch 140/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7604 - val_loss: 0.6806 - val_accuracy: 0.7588\n",
      "Epoch 141/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7604 - val_loss: 0.6805 - val_accuracy: 0.7593\n",
      "Epoch 142/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7604 - val_loss: 0.6809 - val_accuracy: 0.7588\n",
      "Epoch 143/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7603 - val_loss: 0.6808 - val_accuracy: 0.7595\n",
      "Epoch 144/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7603 - val_loss: 0.6808 - val_accuracy: 0.7587\n",
      "Epoch 145/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7602 - val_loss: 0.6806 - val_accuracy: 0.7590\n",
      "Epoch 146/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6741 - accuracy: 0.7603 - val_loss: 0.6805 - val_accuracy: 0.7592\n",
      "Epoch 147/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7603 - val_loss: 0.6807 - val_accuracy: 0.7593\n",
      "Epoch 148/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6806 - val_accuracy: 0.7589\n",
      "Epoch 149/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7603 - val_loss: 0.6806 - val_accuracy: 0.7593\n",
      "Epoch 150/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.7603 - val_loss: 0.6808 - val_accuracy: 0.7587\n",
      "Epoch 151/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6806 - val_accuracy: 0.7589\n",
      "Epoch 152/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6808 - val_accuracy: 0.7588\n",
      "Epoch 153/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6805 - val_accuracy: 0.7592\n",
      "Epoch 154/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6811 - val_accuracy: 0.7593\n",
      "Epoch 155/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7603 - val_loss: 0.6807 - val_accuracy: 0.7593\n",
      "Epoch 156/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6805 - val_accuracy: 0.7593\n",
      "Epoch 157/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6805 - val_accuracy: 0.7593\n",
      "Epoch 158/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6805 - val_accuracy: 0.7592\n",
      "Epoch 159/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7593\n",
      "Epoch 160/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7604 - val_loss: 0.6805 - val_accuracy: 0.7590\n",
      "Epoch 161/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6809 - val_accuracy: 0.7586\n",
      "Epoch 162/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6805 - val_accuracy: 0.7594\n",
      "Epoch 163/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6809 - val_accuracy: 0.7593\n",
      "Epoch 164/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7603 - val_loss: 0.6805 - val_accuracy: 0.7595\n",
      "Epoch 165/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7606 - val_loss: 0.6805 - val_accuracy: 0.7594\n",
      "Epoch 166/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6806 - val_accuracy: 0.7593\n",
      "Epoch 167/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7604 - val_loss: 0.6804 - val_accuracy: 0.7593\n",
      "Epoch 168/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7603 - val_loss: 0.6809 - val_accuracy: 0.7585\n",
      "Epoch 169/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7603 - val_loss: 0.6804 - val_accuracy: 0.7593\n",
      "Epoch 170/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7604 - val_loss: 0.6807 - val_accuracy: 0.7589\n",
      "Epoch 171/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6739 - accuracy: 0.7605 - val_loss: 0.6807 - val_accuracy: 0.7584\n",
      "Epoch 172/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6804 - val_accuracy: 0.7592\n",
      "Epoch 173/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6804 - val_accuracy: 0.7592\n",
      "Epoch 174/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7590\n",
      "Epoch 175/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7592\n",
      "Epoch 176/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6803 - val_accuracy: 0.7592\n",
      "Epoch 177/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6804 - val_accuracy: 0.7592\n",
      "Epoch 178/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7604 - val_loss: 0.6804 - val_accuracy: 0.7591\n",
      "Epoch 179/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7593\n",
      "Epoch 180/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7604 - val_loss: 0.6809 - val_accuracy: 0.7588\n",
      "Epoch 181/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7593\n",
      "Epoch 182/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7607 - val_loss: 0.6805 - val_accuracy: 0.7589\n",
      "Epoch 183/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6803 - val_accuracy: 0.7589\n",
      "Epoch 184/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7604 - val_loss: 0.6803 - val_accuracy: 0.7593\n",
      "Epoch 185/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6803 - val_accuracy: 0.7591\n",
      "Epoch 186/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6804 - val_accuracy: 0.7593\n",
      "Epoch 187/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7604 - val_loss: 0.6804 - val_accuracy: 0.7595\n",
      "Epoch 188/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7606 - val_loss: 0.6814 - val_accuracy: 0.7582\n",
      "Epoch 189/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7604 - val_loss: 0.6812 - val_accuracy: 0.7594\n",
      "Epoch 190/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6803 - val_accuracy: 0.7592\n",
      "Epoch 191/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7605 - val_loss: 0.6804 - val_accuracy: 0.7588\n",
      "Epoch 192/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7604 - val_loss: 0.6803 - val_accuracy: 0.7593\n",
      "Epoch 193/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7604 - val_loss: 0.6802 - val_accuracy: 0.7591\n",
      "Epoch 194/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7604 - val_loss: 0.6802 - val_accuracy: 0.7593\n",
      "Epoch 195/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7604 - val_loss: 0.6809 - val_accuracy: 0.7585\n",
      "Epoch 196/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7604 - val_loss: 0.6802 - val_accuracy: 0.7593\n",
      "Epoch 197/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6807 - val_accuracy: 0.7587\n",
      "Epoch 198/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6803 - val_accuracy: 0.7594\n",
      "Epoch 199/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7589\n",
      "Epoch 200/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6802 - val_accuracy: 0.7591\n",
      "Epoch 201/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6803 - val_accuracy: 0.7589\n",
      "Epoch 202/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7606 - val_loss: 0.6805 - val_accuracy: 0.7595\n",
      "Epoch 203/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7604 - val_loss: 0.6803 - val_accuracy: 0.7593\n",
      "Epoch 204/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7606 - val_loss: 0.6802 - val_accuracy: 0.7591\n",
      "Epoch 205/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6802 - val_accuracy: 0.7590\n",
      "Epoch 206/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6806 - val_accuracy: 0.7589\n",
      "Epoch 207/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.7605 - val_loss: 0.6801 - val_accuracy: 0.7593\n",
      "Epoch 208/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7606 - val_loss: 0.6806 - val_accuracy: 0.7595\n",
      "Epoch 209/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7606 - val_loss: 0.6806 - val_accuracy: 0.7597\n",
      "Epoch 210/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7593\n",
      "Epoch 211/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6801 - val_accuracy: 0.7593\n",
      "Epoch 212/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7606 - val_loss: 0.6802 - val_accuracy: 0.7591\n",
      "Epoch 213/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6806 - val_accuracy: 0.7595\n",
      "Epoch 214/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6802 - val_accuracy: 0.7591\n",
      "Epoch 215/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6805 - val_accuracy: 0.7595\n",
      "Epoch 216/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.7605 - val_loss: 0.6806 - val_accuracy: 0.7588\n",
      "Epoch 217/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7606 - val_loss: 0.6801 - val_accuracy: 0.7591\n",
      "Epoch 218/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6802 - val_accuracy: 0.7589\n",
      "Epoch 219/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6804 - val_accuracy: 0.7594\n",
      "Epoch 220/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6801 - val_accuracy: 0.7594\n",
      "Epoch 221/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6801 - val_accuracy: 0.7594\n",
      "Epoch 222/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7605 - val_loss: 0.6802 - val_accuracy: 0.7595\n",
      "Epoch 223/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7606 - val_loss: 0.6802 - val_accuracy: 0.7590\n",
      "Epoch 224/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7606 - val_loss: 0.6800 - val_accuracy: 0.7592\n",
      "Epoch 225/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7606 - val_loss: 0.6804 - val_accuracy: 0.7592\n",
      "Epoch 226/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7606 - val_loss: 0.6803 - val_accuracy: 0.7592\n",
      "Epoch 227/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7607 - val_loss: 0.6801 - val_accuracy: 0.7594\n",
      "Epoch 228/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7607 - val_loss: 0.6802 - val_accuracy: 0.7594\n",
      "Epoch 229/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7604 - val_loss: 0.6802 - val_accuracy: 0.7595\n",
      "Epoch 230/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7607 - val_loss: 0.6803 - val_accuracy: 0.7595\n",
      "Epoch 231/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6735 - accuracy: 0.7604 - val_loss: 0.6802 - val_accuracy: 0.7594\n",
      "Epoch 232/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7605 - val_loss: 0.6801 - val_accuracy: 0.7591\n",
      "Epoch 233/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7605 - val_loss: 0.6800 - val_accuracy: 0.7593\n",
      "Epoch 234/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7606 - val_loss: 0.6802 - val_accuracy: 0.7590\n",
      "Epoch 235/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7606 - val_loss: 0.6803 - val_accuracy: 0.7594\n",
      "Epoch 236/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7605 - val_loss: 0.6803 - val_accuracy: 0.7592\n",
      "Epoch 237/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7604 - val_loss: 0.6802 - val_accuracy: 0.7590\n",
      "Epoch 238/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7604 - val_loss: 0.6801 - val_accuracy: 0.7592\n",
      "Epoch 239/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7606 - val_loss: 0.6802 - val_accuracy: 0.7591\n",
      "Epoch 240/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7607 - val_loss: 0.6805 - val_accuracy: 0.7586\n",
      "Epoch 241/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7606 - val_loss: 0.6810 - val_accuracy: 0.7596\n",
      "Epoch 242/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7606 - val_loss: 0.6806 - val_accuracy: 0.7587\n",
      "Epoch 243/1000\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7607 - val_loss: 0.6800 - val_accuracy: 0.7593\n",
      "Epoch 00243: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_val, y_train_val, batch_size=1024,\n",
    "            epochs=1000, validation_split=0.25, shuffle=True,\n",
    "            callbacks = [ stopping,reduce_lr ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: model_DNN: 0.7580722891566265\n"
     ]
    }
   ],
   "source": [
    "y_predict, a = Accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "History(model.name, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Implement this mechanism to the starter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "Is the stopping too early? Explore \"learning rate scheduling\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
